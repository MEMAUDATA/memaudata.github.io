{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA FEATURES SELECTION FOR INVESTMENT DECISION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis is based upon the \"nba_dataset.csv\"\n",
    "\n",
    "NV, Toulouse, octobre,2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install all required libraies using requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import joblib as jb \n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For modelisation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, classification_report\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectKBest, f_classif,chi2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PANDAS WARNINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Load data and do the Analysis and visualization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df1 = pd.read_csv(\"nba_logreg.csv\") # warning mac vs windows .// ou .\\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup\n",
    "df = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns\n",
    "pd.set_option(\"display.max.columns\", None)\n",
    "\n",
    "# shape dataset\n",
    "df.shape\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TARGET_5YRS => dependant variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicates / outliers / Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates : Keep first player if multiple players\n",
    "df = df.drop_duplicates(subset='Name', keep=\"first\", inplace=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a specific column with target and values\n",
    "target_col = df['TARGET_5Yrs']\n",
    "name_col    = df['Name']\n",
    "\n",
    "# Labels and data with target for preprocessing\n",
    "labels  = df.drop(['TARGET_5Yrs','Name'],axis=1).columns.values\n",
    "df      = df.drop(['TARGET_5Yrs','Name'],axis=1)\n",
    "df      = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the entire data with heatmap from seaborn to try to look at valuable column\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(df.isna()) # 1 = empty\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group visualization\n",
    "plt.figure()\n",
    "sns.catplot(data=df, kind=\"box\",height = 10)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot column data to find outliers\n",
    "for i in df.columns:\n",
    "    #if col > 0:\n",
    "        plt.figure()\n",
    "        sns.boxplot(df[i])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add target colon to df to remove outliers identification scores\n",
    "df[\"target\"]    = target_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTLIERS FOR THE ENTIRE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice between Zscore / mean +/- + STD / IQR ( last one is more robust) \n",
    "# Calculate Q1 and Q3\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "\n",
    "# Calculate the IQR#\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "upper = Q3 + (1.5 * IQR)\n",
    "lower = Q1 - (1.5 * IQR)\n",
    "\n",
    "\n",
    "# outlier data\n",
    "out= df[(df< upper) & (df>lower)]\n",
    "\n",
    "\n",
    "# Remove outliers with the added target col created earlier\n",
    "print(df.shape)\n",
    "dfwo = df[((df< upper) & (df>lower)).all(axis=1)]\n",
    "print(dfwo.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwo.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTLIERS BY COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers by columns (done it 7 times) untill shape does not change anymore\n",
    "\n",
    "\n",
    "#for k in df.columns:\n",
    "\n",
    "\n",
    "    #Q1 = df[k].quantile(0.25)\n",
    "    #Q3 = df[k].quantile(0.75)\n",
    "\n",
    "    # Calculate the IQR#\n",
    "    #IQR = Q3 - Q1\n",
    "\n",
    "    # limits\n",
    "    #upper = Q3 + (1.5 * IQR)\n",
    "    #lower = Q1 - (1.5 * IQR)\n",
    "\n",
    "    #df = df[(df[k]> lower)& (df[k] < upper)] \n",
    "    #df = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resest index\n",
    "dfwo.index  = range(len(dfwo.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group visualization\n",
    "plt.figure()\n",
    "sns.catplot(data=dfwo, kind=\"box\",height = 10)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nan  values ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(dfwo.isna()) # 1 = empty\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nan values by Zeros !! Why not , probably better than remove the entire column\n",
    "# remove target col\n",
    "dfwo_target_col = dfwo[\"target\"]\n",
    "dfwo = dfwo.drop([\"target\"],axis=1).values # .values to transform from DataFrame to np.array \n",
    "print(type(dfwo))\n",
    "print(dfwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values : NEED NP_ARRAY NOT DATAFRAME\n",
    "for x in np.argwhere(np.isnan(dfwo)):\n",
    "        dfwo[x]=0.0 \n",
    "print(dfwo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back to dataframe\n",
    "dfwo = pd.DataFrame(data = dfwo,columns = labels)\n",
    "# Add target column\n",
    "dfwo[\"target\"] = dfwo_target_col\n",
    "print(dfwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final clean dataset\n",
    "clean_dataset = dfwo \n",
    "print(type(clean_dataset))\n",
    "\n",
    "# save data set with joblib\n",
    "jb.dump(clean_dataset, \"clean_dataset.sav\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleanset = new df without outliers/ nan values / duplicates and an updated target col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTS TO VISUALIZE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean_dataset\n",
    "loaded_model    = jb.load(\"clean_dataset.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup cleanset for plotting purposes\n",
    "pp = loaded_model.copy()\n",
    "pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove target column for plotting purposes\n",
    "pp_target_col  = pp['target']\n",
    "pp             = pp.drop(\"target\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization per groupe but not the same scales therefore try to zscore pp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group visualization\n",
    "plt.figure()\n",
    "sns.catplot(data=pp, kind=\"box\",height = 4)\n",
    "plt.show\n",
    "\n",
    "zcore_pp = pp.apply(stats.zscore)\n",
    "plt.figure()\n",
    "sns.catplot(data=zcore_pp, kind=\"box\",height = 4)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Still some outliers after data reduction but NEED some data for model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "corr_values = pp.corr()\n",
    "corr_heatmap = sns.heatmap(corr_values, xticklabels=df.columns, yticklabels=df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in pp.columns:\n",
    "    x_vect   = np.arange(0,pp.shape[0]) \n",
    "    y_vect   = pp[j]\n",
    "    hue_vect = pp_target_col\n",
    "    labels\n",
    "    f, axes = plt.subplots(1, 2)\n",
    "    plt.title(f'{j}')\n",
    "    sns.kdeplot(data=pp, x=y_vect,hue=pp_target_col,ax = axes[0],palette=\"crest\",alpha=.5,fill=True)\n",
    "    sns.scatterplot(data=pp,x=x_vect, y=y_vect,hue=pp_target_col,ax = axes[1],legend=False,palette=\"crest\",alpha=.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now , keep all datas. Probably the model selection will help us to restrain the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIND MODEL PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train and test set using the clean dataset loaded from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleanset to have a nice clean start\n",
    "clean_dataset    = jb.load('clean_dataset.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataset.copy()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_modelisation(df):\n",
    "\n",
    "    target  = df['target'].to_numpy()\n",
    "    labels  = df.drop(['target'],axis=1).columns.values\n",
    "    df      = df.drop(['target'],axis=1)\n",
    "    data    = df.to_numpy()\n",
    "\n",
    "    return  data,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For score_classifier function\n",
    "data_pp, target_pp    = prepare_data_for_modelisation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL PREDICTION = > OBJECTIVE :  IMPROVE FINAL RECALL SCORE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall scare => lowest False negative  (on a confusion matrix) => model correctly identifying True Positives\n",
    "\n",
    "Before model optimization and features selection we will use the provided function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM             = make_pipeline(MinMaxScaler(),SVC(random_state=0))\n",
    "KNN             = make_pipeline(MinMaxScaler(),KNeighborsClassifier(5))\n",
    "Randomforest    = make_pipeline(RandomForestClassifier(random_state=0)) # Normalization not necessarily need it for trees\n",
    "Adaboost        = make_pipeline(AdaBoostClassifier(algorithm=\"SAMME\",random_state=0))# avoid warning\n",
    "GNB             = make_pipeline(MinMaxScaler(),GaussianNB())\n",
    "\n",
    "\n",
    "dict_of_models  = {\"SVM\" : SVM,\n",
    "                  \"KNN\" : KNN,\n",
    "                  \"Randomforest\" : Randomforest,\n",
    "                  \"Adaboost\" : Adaboost,\n",
    "                  'GNB':GNB}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test score_classifieron the entire clean_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_classifier(dataset,classifier,labels):\n",
    "\n",
    "    \"\"\"\n",
    "    performs 3 random trainings/tests to build a confusion matrix and prints results with precision and recall scores\n",
    "    :param dataset: the dataset to work on\n",
    "    :param classifier: the classifier to use\n",
    "    :param labels: the labels used for training and validation\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    kf = KFold(n_splits=3,random_state=50,shuffle=True) # bootstraping\n",
    "    confusion_mat = np.zeros((2,2))\n",
    "    recall = 0\n",
    "    for training_ids,test_ids in kf.split(dataset):\n",
    "        training_set = dataset[training_ids]\n",
    "        training_labels = labels[training_ids]\n",
    "        test_set = dataset[test_ids]\n",
    "        test_labels = labels[test_ids]\n",
    "        classifier.fit(training_set,training_labels)\n",
    "        predicted_labels = classifier.predict(test_set)\n",
    "        confusion_mat+=confusion_matrix(test_labels,predicted_labels)\n",
    "        recall += recall_score(test_labels, predicted_labels)\n",
    "    recall/=3\n",
    "    print('mean recall out of 3 Kflod splits : ' , recall)\n",
    "    #print(classification_report(test_labels, predicted_labels))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over score_classifier\n",
    "for clf_name, clf in dict_of_models.items():\n",
    "   \n",
    "    print('-----' , clf_name ,'-----')\n",
    "    score_classifier(data_pp, clf, target_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on score_classifier function => choose best algorithm that maximizes recall score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(df, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for future modelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train    = prepare_data_for_modelisation(trainset)\n",
    "X_test, Y_test      = prepare_data_for_modelisation(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize X_train and X_test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler          = MinMaxScaler() # to save it later\n",
    "X_train         = scaler.fit_transform(X_train)\n",
    "X_test          = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model optimization / Hyperparameters fine tuning / features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selected      = SVC()\n",
    "model_selected.fit(X_train, Y_train)\n",
    "Y_pred              = model_selected.predict(X_test)\n",
    "recall              = recall_score(Y_test, Y_pred)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_optimize = make_pipeline(SVC()) \n",
    "hyper_parameters = {'svc__gamma':['auto','scale'],\n",
    "                    'svc__C' : [0.1,1,10,20,100] , \n",
    "                    'svc__kernel':['rbf','linear','sigmoid','poly'],\n",
    "                    'svc__degree':[2,3,4]} \n",
    "\n",
    "grid = RandomizedSearchCV(model_to_optimize, hyper_parameters,scoring='recall',cv=3, n_iter=20)\n",
    "grid.fit(X_train,Y_train)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model optimized based upon Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline(steps=[('svc', SVC(C=0.1, degree=4, gamma='auto', kernel='poly'))]) => Recall of 1 => Too perfect ?\n",
    "\n",
    "model_optimized      = grid.best_estimator_\n",
    "model_optimized.fit(X_train, Y_train)\n",
    "Y_pred              = model_optimized.predict(X_test)\n",
    "recall              = recall_score(Y_test, Y_pred)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features selection and plot using SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features        =  df.drop([\"target\"],axis=1)\n",
    "bestfeatures    = SelectKBest(score_func=f_classif) # choose from figure\n",
    "data_trim       = bestfeatures.fit_transform(X_train, Y_train)\n",
    "filter          = bestfeatures.get_support()\n",
    "print(bestfeatures.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "sns.barplot(x=features.columns,y=bestfeatures.scores_,hue=features.columns,palette=\"crest\",legend=False)\n",
    "plt.title(\"Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose K for SelectKbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features            = df.drop([\"target\"],axis=1)\n",
    "bestfeatures        = SelectKBest(score_func=f_classif,k=3) # choose from figure\n",
    "data_trim           = bestfeatures.fit_transform(X_train, Y_train)\n",
    "filter              = bestfeatures.get_support()\n",
    "\n",
    "selected_features   = features.columns[filter]\n",
    "print(\"selected features : \", list(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDUCE CLEAN_DATASET TO OPTIMIZE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_dataset = df[selected_features]\n",
    "reduce_dataset[\"target\"] = df[\"target\"]\n",
    "print(reduce_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split reduce data again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_reduce_data, testset_reduce_data = train_test_split(reduce_dataset, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rd for reduced_daraset\n",
    "X_train_rd, Y_train_rd    = prepare_data_for_modelisation(trainset_reduce_data)\n",
    "X_test_rd, Y_test_rd      = prepare_data_for_modelisation(testset_reduce_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler              = MinMaxScaler() # to save it later\n",
    "X_train_rd          = scaler.fit_transform(X_train_rd)\n",
    "X_test_rd           = scaler.transform(X_test_rd) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimized      = grid.best_estimator_\n",
    "#model_optimized      = model_selected\n",
    "model_optimized.fit(X_train_rd, Y_train_rd)\n",
    "Y_pred              = model_optimized.predict(X_test_rd)\n",
    "recall              = recall_score(Y_test_rd, Y_pred)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE FINAl OPTIMIZED MODEL WITH RECALL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% save model with joblib \n",
    "jb.dump(scaler, \"scaler.sav\") \n",
    "jb.dump(model_optimized,'nba_model_optimzed.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X           = [[30,1,0.5]]\n",
    "X_scaled    = scaler.transform(X)\n",
    "Y           = model_optimized.predict(X_scaled)\n",
    "print(Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
